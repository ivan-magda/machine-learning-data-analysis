# Переобучение и недообучение

#### Недообучение

*Недообучение* — ситуация, когда алгоритм плохо описывает и обучающую выборку, и новые данные. В этом случае алгоритм необходимо усложнять.

#### Переобучение

В случае *переобучения*, данные из обучающей выборки будут описываться хорошо, а новые данные плохо. Выявить переобучение, используя только обучающую выборку, невозможно, поскольку и хорошо обученный, и переобученный алгоритмы будут хорошо ее описывать. Необходимо использовать дополнительные данные.

- Это излишняя подгонка под обучающую выборку
- Приводит к низкому качуству на новых данных
- Большие веса - признак переобученности линейных моделей

Существуют несколько подходов к выявлению переобучения:
- Отложенная выборка. Часть данных из обучающей выборки не участвуют в обучении, чтобы позже проверять на ней обученный алгоритм
- Кросс-валидация, несколько усложненный метод отложенной выборки
- Использовать меры сложности модели. Об этом пойдет речь далее

# Регуляризация

*Регуляризация* — способ борьбы с переобучением в линейных моделях.

Мерой сложности, то есть «симптомом» переобученности модели, являются большие веса при признаках.
Другая ситуация, в которой можно встретиться с переобучением — мультиколлинеарность. Так называется проблема, при которой признаки в выборке являются линейно зависимыми.

- Регуляризация вводит штраф за большие веса
- L2 - регуляризация - частый выбор
- L1 - регуляризация - сложнее оптимизировать, но можно отбирать признаки

# Коэффициент регуляризации

Введенный коэффициент λ, который стоит перед регуляризатором, называется *коэффициентом регуляризации*. Чем больше λ, тем ниже сложность модели. Например, при очень больших его значениях оптимально просто занулить все веса. В то же время при слишком низких значениях λ высок риск переобучения, то есть модель становится слишком сложной.
Поэтому нужно найти некоторое оптимальное значение λ, достаточно большое, чтобы не допустить переобучения, и не очень большое, чтобы уловить закономерности в данных. Обычно λ подбирается на кросс-валидации&

# Оценивание качества алгоритмов

Самый простой способ оценить качество алгоритма — использование отложенной выборки. В этом случае следует разбить выборку на две части: первая из двух частей будет использоваться для обучения алгоритма, а вторая, тестовая выборка, — для оценки его качества, в том числе для нахождения доли ошибок в задаче классификации, MSE (среднеквадратичной ошибки) в задаче регрессии и других мер качества в зависимости от специфики задачи.

- Для оценивания качества надо использовать данные вне обучения
- Отложенная выборка (когда выборка большая)
- Кросс-валидация (когда выборка маленькая)

# Кросс-валидация

Более системный подход — кросс валидация. В этом случае выборка делится на k блоков примерно одинакового размера. Далее по очереди каждый из этих блоков используется в качестве тестового, а все остальные — в качестве обучающей выборки.


После того, как каждый блок побывает в качестве тестового, будут получены k показателей качества. В результате усреднения получается оценка качества по кросс-валидации.


При этом встает вопрос, какое число блоков использовать. Если блоков мало, получаются надежные, но смещенные оценки. В случае большого числа блоков оценки, наоборот, получаются ненадежными (большой разброс оценок), но несмещенными.


Нет конкретных рекомендаций относительно выбора k. Обычно выбирают k = 3,5,10. Чем больше k, тем больше раз приходится обучать алгоритм. Поэтому на больших выборках следует выбирать небольшие значения k, так как даже при удалении 1/3 выборки (а она большая) оставшихся данных будет достаточно для обучения.

# Сравнение алгоритмов и выбор гиперпараметров

- Гиперпараметры - это те параметры, которые нельзя настроить по обучающей выборке
- При выборе гиперпараметров и сравнении моделей есть риск переобучения
- Надо выделять контрольную выборку

# Случайные леса

#### Композиции деревьев

Проблемы решающих деревьев:
- сильно переобучается
- сильно меняется при небольшом изменении выборки


Композиция деревьев:
- Это объединение n алгоритмов в один.
- Мы каким-то образом нашли N большое алгоритмов b1, ..., bn. Чтобы объединить их в композицию, мы усредняем их ответы, то есть суммируем ответы всех этих алгоритмов b1, ..., bn на объекте x, и делим на N большое, то есть на количество этих алгоритмов.
- Если мы решаем задачу классификации, то далее мы берем знак от этого среднего.
- Если регрессии, то просто возвращаем это среднее как ответ.
- Алгоритм a(x), который возвращает знак среднего или просто среднее и называется *композицией* n алгоритмов.
- Алгоритмы b1, ..., bn, которые мы объединяем в композицию, называются *базовыми алгоритмами*.


Итак, для того чтобы строить композицию, нужно обучить n базовых алгоритмов. При этом понятно, что нельзя их обучать на всей обучающей выборке. Они получатся одинаковыми, и в их усреднении не будет никакого смысла. Нужно делать их немного различными. Например, с помощью **рандомизации**, то есть обучать их по разным подвыборкам обучающей выборки.


Поскольку решающие деревья очень сильно меняются даже при небольших изменениях обучающей выборки, такая рандомизация с помощью подвыборок будет очень хорошо влиять на их различность. 


Подходы к рандомизации:
- [Бутстрэп](https://habrahabr.ru/company/ods/blog/324402/#butstrep)
- Генерация случайного подмножества обучающей выборки

#### Смещение и разброс
- **Смещение** показывает, насколько сильно отклоняется средний прогноз по всем обученным моделям от прогноза идеальной модели. По сути, смещение говорит, насколько мы можем аппроксимировать идеальную модель, насколько наше семейство алгоритмов сложное и позволяет восстанавливать сложные закономерности.
- **Разброс** - дисперсия ответов наших моделей. Чем больше эта дисперсия, чем выше разброс, тем сильней алгоритм зависит от небольших изменений обучающей выборки. Например, решающие деревья обладают этим свойством. Если чуть-чуть поменять обучающую выборку, дерево меняется очень сильно.


 Смещение линейных модели может быть довольно большим. Линейные модели могут восстанавливать только линейные зависимости, но при этом в большинстве задач зависимости нелинейные, из за чего смещение большое и линейная модель в принципе не может восстановить сложные зависимости. При этом разброс маленький. У линейной модели параметров столько, сколько признаков. Это очень мало. Вряд ли они сильно изменятся, если чуть-чуть поменять обучающую выборку. Итак, у **линейных моделей большое смещение и низкий разброс**.


 Решающие деревья — это полная противоположность. У них **низкое смещение**. Они могут восстанавливать очень сложные закономерности, но при этом **разброс** очень большой, поскольку деревья очень сильно меняются даже при небольших изменениях обучающей выборки. 


- Ошибка складывается из смещения и разброса
- Усреднение алгоритмов не меняет смещение и уменьшает разброс
- Чем меньше корреляция между ответами базовых алгоритмов, тем сильнее уменьшение разброса
- Бэггинг и метод случайных подпространств


Смещение характеризует, насколько сложно у нас семейство алгоритмов, насколько оно может восстанавливать сложные закономерности, а разброс говорит, насколько чувствительны алгоритмы к небольшим изменениям выборки.
При этом мы выяснили, что усреднение алгоритмов, объединение их в такую композицию не меняет смещение и при этом уменьшает разброс.
При этом чем менее коррелированы базовые алгоритмы, тем сильнее уменьшение разброса.
И обсудили два подхода к уменьшению корреляции между базовыми алгоритмами: бэггинг и метод случайных подпространств.
